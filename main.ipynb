{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 9.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\n",
      "ERROR: No matching distribution found for PIL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytesseract) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.10\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.16.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdf2image) (9.2.0)\n",
      "Installing collected packages: pdf2image\n",
      "Successfully installed pdf2image-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PIL\n",
    "!pip install pytesseract\n",
    "!pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.13.0-py2.py3-none-any.whl (9.2 MB)\n",
      "     ---------------------------------------- 9.2/9.2 MB 10.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.4 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from streamlit) (2.28.1)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from streamlit) (6.2)\n",
      "Collecting tzlocal>=1.1\n",
      "  Using cached tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from streamlit) (9.2.0)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting blinker>=1.0.0\n",
      "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
      "Collecting altair>=3.2.0\n",
      "  Downloading altair-4.2.0-py3-none-any.whl (812 kB)\n",
      "     ------------------------------------- 812.8/812.8 kB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from streamlit) (8.1.3)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "     -------------------------------------- 237.5/237.5 kB 7.3 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=4.0\n",
      "  Downloading pyarrow-9.0.0-cp310-cp310-win_amd64.whl (19.5 MB)\n",
      "     --------------------------------------- 19.5/19.5 MB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from streamlit) (5.2.0)\n",
      "Requirement already satisfied: pandas>=0.21.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from streamlit) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from streamlit) (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from streamlit) (1.23.3)\n",
      "Requirement already satisfied: protobuf!=3.20.2,<4,>=3.12 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from streamlit) (3.19.6)\n",
      "Collecting watchdog\n",
      "  Downloading watchdog-2.1.9-py3-none-win_amd64.whl (78 kB)\n",
      "     ---------------------------------------- 78.4/78.4 kB ? eta 0:00:00\n",
      "Collecting importlib-metadata>=1.4\n",
      "  Downloading importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: packaging>=14.1 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from streamlit) (21.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from streamlit) (2.8.2)\n",
      "Collecting gitpython!=3.1.19\n",
      "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
      "     -------------------------------------- 182.5/182.5 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting pympler>=0.9\n",
      "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "     -------------------------------------- 164.8/164.8 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting validators>=0.2\n",
      "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.8.0b4.dev1-py2.py3-none-any.whl (4.7 MB)\n",
      "     ---------------------------------------- 4.7/4.7 MB 10.7 MB/s eta 0:00:00\n",
      "Collecting semver\n",
      "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting jsonschema>=3.0\n",
      "  Downloading jsonschema-4.16.0-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.1/83.1 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 55.8/55.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: entrypoints in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from altair>=3.2.0->streamlit) (0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from click>=7.0->streamlit) (0.4.5)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.9.0-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=14.1->streamlit) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.21.0->streamlit) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil->streamlit) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.4->streamlit) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.4->streamlit) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.4->streamlit) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.4->streamlit) (3.4)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "     ---------------------------------------- 51.1/51.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.11.0->streamlit) (2.13.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tzlocal>=1.1->streamlit) (2022.4)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\anude\\appdata\\roaming\\python\\python310\\site-packages (from validators>=0.2->streamlit) (5.1.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->altair>=3.2.0->streamlit) (2.1.1)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Downloading pyrsistent-0.18.1-cp310-cp310-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.6/61.6 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\anude\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (22.1.0)\n",
      "Building wheels for collected packages: validators\n",
      "  Building wheel for validators (setup.py): started\n",
      "  Building wheel for validators (setup.py): finished with status 'done'\n",
      "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=dbe117cb4b98cee6ac47b4a0da5ebfe74b6c9d0db7808e5cbb88cf69055cf9d1\n",
      "  Stored in directory: c:\\users\\anude\\appdata\\local\\pip\\cache\\wheels\\f2\\ed\\dd\\d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n",
      "Successfully built validators\n",
      "Installing collected packages: commonmark, zipp, watchdog, validators, toolz, toml, smmap, semver, rich, pytz-deprecation-shim, pyrsistent, pympler, pyarrow, blinker, tzlocal, pydeck, jsonschema, importlib-metadata, gitdb, gitpython, altair, streamlit\n",
      "Successfully installed altair-4.2.0 blinker-1.5 commonmark-0.9.1 gitdb-4.0.9 gitpython-3.1.29 importlib-metadata-5.0.0 jsonschema-4.16.0 pyarrow-9.0.0 pydeck-0.8.0b4.dev1 pympler-1.0.1 pyrsistent-0.18.1 pytz-deprecation-shim-0.1.0.post0 rich-12.6.0 semver-2.13.0 smmap-5.0.0 streamlit-1.13.0 toml-0.10.2 toolz-0.12.0 tzlocal-4.2 validators-0.20.0 watchdog-2.1.9 zipp-3.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n",
      "\n",
      "Some layers of TFPegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['final_logits_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\anude/.cache\\huggingface\\hub\\models--google--pegasus-xsum\\snapshots\\c4455b515a3ed63d76e9ff476a78dde8d0484239\\pytorch_model.bin' at 'C:\\Users\\anude/.cache\\huggingface\\hub\\models--google--pegasus-xsum\\snapshots\\c4455b515a3ed63d76e9ff476a78dde8d0484239\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:371\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 371\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    372\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m--> 713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:938\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    937\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m--> 938\u001b[0m typed_storage\u001b[39m.\u001b[39;49m_storage\u001b[39m.\u001b[39;49m_set_from_file(\n\u001b[0;32m    939\u001b[0m     f, offset, f_should_read_directly,\n\u001b[0;32m    940\u001b[0m     torch\u001b[39m.\u001b[39;49m_utils\u001b[39m.\u001b[39;49m_element_size(typed_storage\u001b[39m.\u001b[39;49mdtype))\n\u001b[0;32m    941\u001b[0m \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unexpected EOF, expected 348101121 more bytes. The file might be corrupted.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:375\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 375\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39;49mread()\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    376\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    378\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    379\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou cloned.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    380\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 2097: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m torch_device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m tokenizer \u001b[39m=\u001b[39m PegasusTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 12\u001b[0m model \u001b[39m=\u001b[39m PegasusForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(model_name)\u001b[39m.\u001b[39mto(torch_device)\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2110\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2107\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   2108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2109\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2110\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   2112\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2113\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2114\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2115\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2116\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2117\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:387\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to locate the file \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m which is necessary to load this pretrained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mUnicodeDecodeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    390\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\anude/.cache\\huggingface\\hub\\models--google--pegasus-xsum\\snapshots\\c4455b515a3ed63d76e9ff476a78dde8d0484239\\pytorch_model.bin' at 'C:\\Users\\anude/.cache\\huggingface\\hub\\models--google--pegasus-xsum\\snapshots\\c4455b515a3ed63d76e9ff476a78dde8d0484239\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\"\n",
    "The two armies had gathered on the battlefield of Kurukshetra, well prepared to fight a war that was inevitable. Still, in this verse, King Dhritarashtra asked Sanjay, what his sons and his brother Pandu’s sons were doing on the battlefield? It was apparent that they would fight, then why did he ask such a question?\n",
    "\n",
    "The blind King Dhritarashtra’s fondness for his own sons had clouded his spiritual wisdom and deviated him from the path of virtue. He had usurped the kingdom of Hastinapur from the rightful heirs; the Pandavas, sons of his brother Pandu. Feeling guilty of the injustice he had done towards his nephews, his conscience worried him about the outcome of this battle.\n",
    "\n",
    "The words dharma kṣhetre, the land of dharma (virtuous conduct) used by Dhritarashtra depict the dilemma he was experiencing.  Kurukshetra is described as kurukṣhetraṁ deva yajanam in the Shatapath Brahman, the Vedic textbook detailing rituals. It means “Kurukshetra is the sacrificial arena of the celestial gods.” Hence, it was regarded as the sacred land that nourished dharma. \n",
    "\n",
    "Dhritarashtra feared that the holy land might influence the minds of his sons. If it aroused the faculty of discrimination, they might turn away from killing their cousins and negotiate a truce. A peaceful settlement meant that the Pandavas would continue being a hindrance for them. He felt great displeasure at these possibilities, instead preferred that this war transpires. He was uncertain of the consequences of the war, yet desired to determine the fate of his sons. Therefore, he asked Sanjay about the activities of the two armies on the battleground.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two armies had gathered on the battlefield of Kurukshetra, well prepared to fight a war that was inevitable. King Dhritarashtra feared that the holy land might influence the minds of his sons. If it aroused the faculty of discrimination, they might turn away from killing their cousins and negotiate a truce.\n",
      "==================================================================================\n",
      "Length Before Summarization: 1630\n",
      "Length After Summarization: 312\n",
      "Article Reduced By: 81%\n",
      "Curently Using: Facebook's BART-large-cnn model from HuggingFace library\n"
     ]
    }
   ],
   "source": [
    "text = text[0]['summary_text']\n",
    "print(text)\n",
    "print(\"==================================================================================\")\n",
    "print(\"Length Before Summarization:\", len(ARTICLE))\n",
    "print(\"Length After Summarization:\", len(text))\n",
    "print(f\"Article Reduced By: {100 - round((len(text)/len(ARTICLE))*100)}%\")\n",
    "print(\"Curently Using: Facebook's BART-large-cnn model from HuggingFace library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results and Summary to a json file\n",
    "import json\n",
    "\n",
    "saves = {\n",
    "    \"Article\": ARTICLE,\n",
    "    \"Summary\": text,\n",
    "    \"Length Before Summarization\": len(ARTICLE),\n",
    "    \"Length After Summarization\": len(text),\n",
    "    \"Article Reduced By\": f\"{100 - round((len(text)/len(ARTICLE))*100)}%\",\n",
    "    \"Curently Using\": \"Facebook's BART-large-cnn model from HuggingFace library\"\n",
    "}\n",
    "\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(saves, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1062: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deep-learning-analytics/wikihow-t5-small\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"deep-learning-analytics/wikihow-t5-small\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"\n",
    "Thiruvananthapuram: Kerala Governor Arif Mohammed Khan has written to Chief Minister Pinarayi Vijayan, demanding the dismissal of Finance Minister KN Balagopal over recent remarks at a university, which he said were \"seditious\".\n",
    "In his letter to the Chief Minister, the Governor alleged that Mr Balagopal delivered a speech at a campus in Thiruvananthapuram last week, seeking to \"stoke the fire of regionalism and provincialism and undermining the unity of India\".\n",
    "\n",
    "According to reports, minister KN Balagopal had said at the function that \"those who come from places like UP, may find it tough to understand universities in Kerala\", recalling violent crackdowns on students by authorities in other parts of the country.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text = text.strip().replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summarized text: \n",
      " Talk to the Chief Minister.\n",
      "Percentage Reduction:  96 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "            tokenized_text,\n",
    "            max_length=150, \n",
    "            num_beams=2,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nSummarized text: \\n\", output)\n",
    "print(\"Percentage Reduction: \", 100 - round((len(output)/len(preprocess_text))*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anude\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 142 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the committee will function in place of the congress's working committee, the highest decision - making body of the party. new members of the working committee will be chosen at the next aicc ( all - india congress committee ) session. this is part of the convention when a new chief is elected, to enable him to choose his team.\n",
      "Percentage Reduction:  44 %\n",
      "Time Taken:  16.3928005695343\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
    "import torch\n",
    "import time\n",
    "start = time.time()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization')\n",
    "model = EncoderDecoderModel.from_pretrained('mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization').to(device)\n",
    "\n",
    "def generate_summary(text):\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  \n",
    "text = \"\"\"New Delhi: New Congress chief Mallikarjun Kharge has formed a steering committee with 47 members including Sonia Gandhi, Manmohan Singh and Rahul Gandhi. The committee will function in place of the Congress's Working Committee, the highest decision-making body of the party, till the plenary session is held. New members of the Working Committee will be chosen at the next AICC (All-India Congress Committee) session.  \n",
    "This morning, the members of the Congress Working Committee stepped down. This is part of the convention when a new chief is elected, to enable him to choose his team.  \"\"\"\n",
    "print(generate_summary(text))\n",
    "print(\"Percentage Reduction: \", 100 - round((len(generate_summary(text))/len(text))*100), \"%\")\n",
    "print(\"Time Taken: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cab22fcadfb19adb896dda6e7bb1fb5c29bda4cc72d7992e0122f4af813fcc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
